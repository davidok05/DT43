import nltk
from collections import Counter
from nltk.tokenize import word_tokenize

# Lejupielādēt nepieciešamos resursus
nltk.download('punkt')

# Teksts
text = "Mākoņainā dienā kaķis sēdēja uz palodzes. Kaķis domāja, kāpēc debesis ir pelēkas. Kaķis gribēja redzēt sauli, bet saule slēpās aiz mākoņiem."

# Teksta tokenizācija
tokens = word_tokenize(text.lower())  # Pārveidot uz maziem burtiem un tokenizēt

# Vārdu biežuma skaitīšana
word_freq = Counter(tokens)

# Izfiltrēt tikai vārdus (ignorēt pieturzīmes)
filtered_word_freq = {word: freq for word, freq in word_freq.items() if word.isalpha()}

# Rezultāta izdruka
print("Vārdu biežums:")
for word, freq in filtered_word_freq.items():
    print(f"{word}: {freq}")
